---
layout: page
title: Projects
permalink: /projects/
---
<p align="justify">

<b>Computation Student Intern, Lawrence Livermore National Laboratory (Jun - Sep 2017):</b>
                <ul>
                        <li><b><a class="page-link"  href="/influence_analysis_performance"><u>Influence Analysis for Performance Data:</u></a></b> Performed a novel, model-agnostic influence analysis using Graph Signal Processing to quantify parameter and data
sample influence on the performance of a High Performance Computing application (Python)</li>
                                <ul>
                                        <li>Non-black-box method whose results aligned with the intuition of HPC experts (and to be tested on more apps)</li>
                                        <li>Validated results with Neural Networks, Gradient Boosting, Random Forests (Keras, scikit-learn, TensorFlow)</li>
					<li>Selected to present a poster summarizing our findings and methodology at the premier Supercomputing conference <a href="https://sc17.supercomputing.org">SC17</a></li>
                                </ul>
                        <li><b><a class="page-link" href="/graph_signal_processing"><u>A Graph Signal Processing Approach for Sample Influence Analysis:</u></a></b> Identified influential samples and computed image saliency maps in <a href="https://www.kaggle.com/c/dogs-vs-cats">Kaggle </a>and <a href="http://www.image-net.org">ImageNet</a> datasets using <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet’s</a> latent space (with/without fine-tuning) using Graph Signal Processing (submitted to ICML 2018 - arXiv link <a class="page-link" href="http://arxiv.org/abs/1711.05407 "><b><u>here</u></b></a>) </li>
				<ul>
					<li>Presented a poster summarizing our methodology and findings at the <a href="http://sites.google.com/view/socalml17/home">SoCal Machine Learning Symposium 2017</a></li>
				</ul>
                </ul>
</p>

### <b> Other Projects </b>
<p>
<ul>
	<li><a class="page-link" href = "/facial_expression_recognition"><b><u>Facial Expression Recognition:</u></b></a> Created a gradient boosted ensemble of Convolutional Neural Networks and a K-nearest neighbors model after reducing dimensionality using Principal Components Analysis to predict facial expressions (49% accuracy on 7-class data) (TensorFlow, scikit-learn, GraphLab)<br></li>
	<li><a class="page-link" href = "/search_engine"><b><u>Search Engine:</u></b></a> Built a search engine with a web interface containing modules for tokenizing, indexing, and ranking webpages in UC Irvine's website (using PageRank, tf-idf, bigrams and tag-based attributes) (Python)<br></li>
	<li><a class="page-link" href = "/compiler_vectorization_prediction"> <b><u>Compiler Vectorization Prediction:</u></b></a> Improved the prediction accuracy of compiler vectorization by 6% using Random Forests and K-nearest neighbors after performing data augmentation with SMOTE and synthpop<br></li>
	<li><a class="page-link" href = "/rainfall_prediction"><b><u>Rainfall Prediction (Class Kaggle project):</u></b></a> Predicted the probability of rainfall in a location using Random Forests, Logistic Regression, Boosted Decision Trees and Neural Networks in Python (model performance in top ~20% of class)<br></li>
	<li><a class="page-link" href = "/sokoban_solver"><b><u>Sokoban Solver:</u></b></a> Implemented a system for solving the Sokoban puzzle using various search strategies (A*, Iterative Deepening A*, Breadth First Search) and heuristics (such as the Manhattan distance) in C++<br></li>
	<li><a class="page-link" href = "/emotion_identification"><b><u>Emotion Identification of Songs:</u></b></a> Predicted the emotions of songs using their audio and lyrical content with Support Vector Machines, Naive Bayes, Random Forests, and word lists in Python and Weka<br></li>
</ul>
</p>
### <b>Other Work Experience:</b>
<p>
<b>Decision Scientist, Mu Sigma (Aug 2014 - Apr 2016): </b>
		<ul>
			<li>Devised and implemented a solution to recommend the optimal marketing expenditure allocation for an Australian
insurer using multiplicative models, regression and nonlinear optimization on time series data (in R)</li>
				<ul>
					<li>Projected savings of ~7% ($2M) of annual marketing budget (recommendations were tested from Jul ‘16)</li>
					<li>Won the “D-WOW” award, a highly selective recognition awarded by the CEO to outstanding projects</li>
					<li>Awarded the “Impact Award” for showing utmost ownership, dependability and grit in all endeavors</li>
				</ul>
			<li>Developed a solution to predict fraudulent insurance claims and increase the efficiency of insurance claims investigators using Exploratory Data Analysis, Logistic Regression and clustering techniques (in R)</li>
			<li>Served as Teaching Assistant for our hands-on introduction to data science program (for new employees)</li>
		</ul>

</p>
